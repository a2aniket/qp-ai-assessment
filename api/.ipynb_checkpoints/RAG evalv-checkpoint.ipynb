{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0f44e68-62f3-4524-8214-c6d51ffe39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4091cdb-4672-4dec-a58e-4eaf84f47f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"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\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"langchain-project\"\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "EMBEDDING_PATH = \"C:/code/qp-ai-assessment/api/misc/embeddings/\"\n",
    "MODEL_KWARGS = {\"device\": \"cpu\"}\n",
    "ENCODE_KWARGS = {\"normalize_embeddings\": True}\n",
    "\n",
    "\n",
    "\n",
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs=MODEL_KWARGS,\n",
    "    encode_kwargs=ENCODE_KWARGS\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14568174-dcd1-43f0-9f9f-18f490474d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"What are some key achievements and recognitions in Aniket Thorat's professional career?\",\n",
    "    \"What are Aniket Thorat's core competencies in AI and machine learning?\",\n",
    "    \"What is Aniket Thorat's current role and responsibilities at Persistent Systems?\",\n",
    "    \"What educational background and certifications does Aniket Thorat have?\",\n",
    "    \"Can you describe some of the projects Aniket Thorat has undertaken?\"\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"Aniket Thorat has received several notable achievements and recognitions: 1. Topper in GEMS category for Persistent 2022 batch. 2. Multiple Bravo Awards from CTO for exceptional project contributions. 3. Best Project Honor in February 2019 by Rotary Club of Poona West. 4. Project with Best Social Impact in March 2019 by ZEAL Institutes. 5. Conducts AI tools workshops for clients and internal teams. These highlight his technical expertise, project success, and commitment to knowledge sharing in AI and software engineering.\",\n",
    "\n",
    "    \"Aniket Thorat's core competencies in AI and machine learning include: 1. Machine Learning & Deep Learning. 2. Data Preprocessing. 3. Natural Language Processing. 4. Neural Network Architecture Design. 5. Model Deployment & Optimization. 6. AI Frameworks & Libraries. 7. Computer Vision. 8. Algorithm Development. 9. Data Analysis & Visualization. These skills demonstrate his comprehensive expertise in various aspects of AI and machine learning.\",\n",
    "\n",
    "    \"At Persistent Systems, Aniket Thorat is a Senior Software Engineer. His responsibilities include: 1. Designing and implementing AI-driven solutions for complex customer challenges. 2. Developing AI Ops applications and collaborating with hyperscale providers. 3. Working with pre-sales and delivery excellence teams to demonstrate AI capabilities. 4. Evaluating emerging technologies to improve developer productivity. 5. Managing the inter-company Knowledge Traverser Platform. 6. Designing and implementing structured knowledge graphs for unstructured data exploration.\",\n",
    "\n",
    "    \"Aniket Thorat's educational background and certifications include: 1. B.E. (Computer Engineering) from Sinhgad Institute of Technology and Science (SITS), completed in 2022. 2. Data Science Specialization certification from IBM, obtained in 2022. This combination of formal education and specialized certification demonstrates his strong foundation in computer engineering and data science.\",\n",
    "\n",
    "    \"Aniket Thorat has undertaken several significant projects: 1. Production Failure and Root Cause Analysis (RCA) Automation: Automated prediction and identification of production failures to minimize downtime and enhance system reliability. 2. MingMate (14 Weeks): Improved developer productivity by integrating AI and Generative AI into software development processes. 3. Root Navigation for Blind Person (5 Months): Developed an assistant tool to help visually impaired individuals navigate and find routes. These projects showcase his ability to apply AI solutions to diverse real-world problems.\"\n",
    "]\n",
    "\n",
    "\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "\n",
    "# Create dataset\n",
    "client = Client()\n",
    "dataset_name = \"RAG_test_LCEL22\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about LCEL.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0eadbec5-de8c-42bb-84ba-821f19123432",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INDEX\n",
    "'''\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load docs\n",
    "url = \"https://python.langchain.com/v0.1/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4500, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed and store in Chroma\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=huggingface_embeddings)\n",
    "\n",
    "# Index\n",
    "retriever = vectorstore.as_retriever()\n",
    "'''\n",
    "vectorstore = FAISS.load_local(\"C:/code/qp-ai-assessment/api/misc/embeddings\", huggingface_embeddings,allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a19c37f-01a0-49af-966c-801d94a161e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.schema import Document\n",
    "from langsmith import traceable\n",
    "\n",
    "class RagBot:\n",
    "    def __init__(self, retriever, repo_id: str = \"mistralai/Mistral-7B-v0.1\"):\n",
    "        self._retriever = retriever\n",
    "        self._llm = HuggingFaceHub(\n",
    "            repo_id=repo_id,\n",
    "            model_kwargs={\"temperature\": 0.1, \"max_length\": 500}\n",
    "        )\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.get_relevant_documents(question)\n",
    "\n",
    "    @traceable()\n",
    "    def invoke_llm(self, question: str, docs: List[Document]):\n",
    "        docs_content = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        system_prompt = (\n",
    "            \"You are a helpful AI code assistant with expertise in LCEL. \"\n",
    "            \"Use the following docs to produce a concise code solution to the user question.\\n\\n\"\n",
    "            f\"## Docs\\n\\n{docs_content}\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        input_text = f\"{system_prompt}Human: {question}\\nAssistant:\"\n",
    "        \n",
    "        response = self._llm(input_text)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response,\n",
    "            \"contexts\": [doc.page_content for doc in docs],\n",
    "        }\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        docs = self.retrieve_docs(question)\n",
    "        return self.invoke_llm(question, docs)\n",
    "\n",
    "# Initialize the RagBot with a retriever\n",
    "rag_bot = RagBot(retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd89d70e-0579-4da1-ac3e-a84be4899719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful AI code assistant with expertise in LCEL. Use the following docs to produce a concise code solution to the user question.\\n\\n## Docs\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_bot.get_answer(\"What is LCEL?\")\n",
    "response[\"answer\"][:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7be8cdd3-02a6-49d4-8ac1-ab9fbeeaabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain\n",
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4217f3a-5873-49ff-a75e-0e2bdc356d57",
   "metadata": {},
   "source": [
    "### Reference Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd176e05-644e-4b1f-bf14-33b8e6c6cd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-mistral-79ab9e2e' at:\n",
      "https://smith.langchain.com/o/b59f3614-9d00-5c85-a713-5c5f2675f256/datasets/e4013450-3b90-4ce2-8f20-1e24f36e61ff/compare?selectedSessions=f23a9953-6be4-43b3-91c2-2d812280dd2b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2e211de4234e9fbc0941b263942ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "from langchain.llms import HuggingFaceEndpoint\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# Define the LLM\n",
    "\n",
    "\n",
    "# Evaluator\n",
    "qa_evaluator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\",\n",
    "        config={\"llm\": llm},  # Pass the LLM to the evaluator\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"],\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"input\": example.inputs[\"question\"],\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "#dataset_name = \"RAG_test_LCEL\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"rag-qa-mistral\",\n",
    "    metadata={\"variant\": \"LCEL context, Mistral-7B\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2690e-6e25-4807-8e32-d7532b52f397",
   "metadata": {},
   "source": [
    "### Answer Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "462fa5e7-351a-4a7b-8a3b-ec0cf56ef7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "\n",
    "answer_hallucination_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"\"\"Is the Assistant's Answer grounded in the Ground Truth documentation? A score of [[1]] means that the\n",
    "            Assistant answer contains is not at all based upon / grounded in the Ground Truth documentation. A score of [[5]] means \n",
    "            that the Assistant answer contains some information (e.g., a hallucination) that is not captured in the Ground Truth \n",
    "            documentation. A score of [[10]] means that the Assistant answer is fully based upon the in the Ground Truth documentation.\"\"\"\n",
    "        },\n",
    "        \"llm\": llm,  # Explicitly pass the Hugging Face model here\n",
    "        # If you want the score to be saved on a scale from 0 to 1\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"contexts\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n",
    "dataset_name = \"RAG_test_LCEL22\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai-hallucination\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"LCEL context, gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6fcb0b2-c447-425c-9d93-e273b6c442b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-hallucination-06a72931' at:\n",
      "https://smith.langchain.com/o/b59f3614-9d00-5c85-a713-5c5f2675f256/datasets/e4013450-3b90-4ce2-8f20-1e24f36e61ff/compare?selectedSessions=79d79738-4051-411b-a939-bf69987c60e9\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5c5bfb9840462aad4e13e9ce544671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fb2a2f6-4d9d-4ec0-8f4f-f7edfbd2467c",
   "metadata": {},
   "source": [
    "### Document Relevance to Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d7fff90-5a71-4a31-9dfd-250b22d4963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "import textwrap\n",
    "\n",
    "docs_relevance_evaluator = LangChainStringEvaluator(\n",
    "    \"score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"document_relevance\": textwrap.dedent(\n",
    "                \"\"\"The response is a set of documents retrieved from a vectorstore. The input is a question\n",
    "            used for retrieval. You will score whether the Assistant's response (retrieved docs) is relevant to the Ground Truth \n",
    "            question. A score of [[1]] means that none of the  Assistant's response documents contain information useful in answering or addressing the user's input.\n",
    "            A score of [[5]] means that the Assistant answer contains some relevant documents that can at least partially answer the user's question or input. \n",
    "            A score of [[10]] means that the user input can be fully answered using the content in the first retrieved doc(s).\"\"\"\n",
    "            )\n",
    "        },\n",
    "        \"llm\": llm,\n",
    "        # If you want the score to be saved on a scale from 0 to 1\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"contexts\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "dataset_name = \"RAG_test_LCEL22\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[docs_relevance_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai-doc-relevance\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"LCEL context, gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19c730b3-b13d-4239-ad81-ebcd78c7e3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-oai-doc-relevance-579f0392' at:\n",
      "https://smith.langchain.com/o/b59f3614-9d00-5c85-a713-5c5f2675f256/datasets/e4013450-3b90-4ce2-8f20-1e24f36e61ff/compare?selectedSessions=6785e49e-2b66-4875-b099-83a46ea144bb\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f31ac7ce894a6489df161fe14e623b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4da58ecc-c308-467e-ac99-d79770331d2c",
   "metadata": {},
   "source": [
    "### Evaluating intermediate traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3c6f8b5-0dd4-4e5f-8165-62b708c00855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'LCEL context-b816af4e' at:\n",
      "https://smith.langchain.com/o/b59f3614-9d00-5c85-a713-5c5f2675f256/datasets/e4013450-3b90-4ce2-8f20-1e24f36e61ff/compare?selectedSessions=f035bb9e-193a-4708-80d8-5c0762bfd45b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49b13e08ea24835bb0bb3ffe1083251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'GradeDocuments', 'description': 'Binary score for relevance check on retrieved documents.', 'parameters': {'type_': 6, 'properties': {'binary_score': {'type_': 3, 'description': 'Documents are relevant to the question, 1 or 0', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['binary_score'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'GradeDocuments', 'description': 'Binary score for relevance check on retrieved documents.', 'parameters': {'type_': 6, 'properties': {'binary_score': {'type_': 3, 'description': 'Documents are relevant to the question, 1 or 0', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['binary_score'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'GradeDocuments', 'description': 'Binary score for relevance check on retrieved documents.', 'parameters': {'type_': 6, 'properties': {'binary_score': {'type_': 3, 'description': 'Documents are relevant to the question, 1 or 0', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['binary_score'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'GradeDocuments', 'description': 'Binary score for relevance check on retrieved documents.', 'parameters': {'type_': 6, 'properties': {'binary_score': {'type_': 3, 'description': 'Documents are relevant to the question, 1 or 0', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['binary_score'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'GradeDocuments', 'description': 'Binary score for relevance check on retrieved documents.', 'parameters': {'type_': 6, 'properties': {'binary_score': {'type_': 3, 'description': 'Documents are relevant to the question, 1 or 0', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['binary_score'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'GradeHallucinations', 'description': 'Binary score for hallucination present in generation answer.', 'parameters': {'type_': 6, 'properties': {'binary_score': {'type_': 3, 'description': 'Answer is grounded in the facts, 1 or 0', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['binary_score'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'GradeHallucinations', 'description': 'Binary score for hallucination present in generation answer.', 'parameters': {'type_': 6, 'properties': {'binary_score': {'type_': 3, 'description': 'Answer is grounded in the facts, 1 or 0', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['binary_score'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'GradeHallucinations', 'description': 'Binary score for hallucination present in generation answer.', 'parameters': {'type_': 6, 'properties': {'binary_score': {'type_': 3, 'description': 'Answer is grounded in the facts, 1 or 0', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['binary_score'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'GradeHallucinations', 'description': 'Binary score for hallucination present in generation answer.', 'parameters': {'type_': 6, 'properties': {'binary_score': {'type_': 3, 'description': 'Answer is grounded in the facts, 1 or 0', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['binary_score'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'GradeHallucinations', 'description': 'Binary score for hallucination present in generation answer.', 'parameters': {'type_': 6, 'properties': {'binary_score': {'type_': 3, 'description': 'Answer is grounded in the facts, 1 or 0', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['binary_score'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "def document_relevance_grader(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator that checks to see if retrieved documents are relevant to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # Get documents and question\n",
    "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"get_answer\")\n",
    "    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieve_docs\")\n",
    "    doc_txt = \"\\n\\n\".join(doc.page_content for doc in retrieve_run.outputs[\"output\"])\n",
    "    question = retrieve_run.inputs[\"question\"] \n",
    "\n",
    "    # Data model for grade\n",
    "    class GradeDocuments(BaseModel):\n",
    "        \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "        binary_score: int = Field(description=\"Documents are relevant to the question, 1 or 0\")\n",
    "    \n",
    "    # LLM with function call \n",
    "    structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "    \n",
    "    # Prompt \n",
    "    system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Give a binary score 1 or 0 score, where 1 means that the document is relevant to the question.\"\"\"\n",
    "    \n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    retrieval_grader = grade_prompt | structured_llm_grader\n",
    "    score = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "    return {\"key\": \"document_relevance\", \"score\": int(score.binary_score)}\n",
    "\n",
    "def answer_hallucination_grader(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator that checks to see the answer is grounded in the documents\n",
    "    \"\"\"\n",
    "\n",
    "    # Get documents and answer\n",
    "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"get_answer\")\n",
    "    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieve_docs\")\n",
    "    doc_txt = \"\\n\\n\".join(doc.page_content for doc in retrieve_run.outputs[\"output\"])\n",
    "    generation = rag_pipeline_run.outputs[\"answer\"]\n",
    "    \n",
    "    # Data model\n",
    "    class GradeHallucinations(BaseModel):\n",
    "        \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    \n",
    "        binary_score: int = Field(description=\"Answer is grounded in the facts, 1 or 0\")\n",
    "    \n",
    "    # LLM with function call \n",
    "    structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "    \n",
    "    # Prompt \n",
    "    system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "         Give a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "    hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "    score = hallucination_grader.invoke({\"documents\": doc_txt, \"generation\": generation})\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": int(score.binary_score)}\n",
    "\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "dataset_name = \"RAG_test_LCEL22\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[document_relevance_grader,answer_hallucination_grader],\n",
    "    experiment_prefix= \"LCEL context\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a259869-220d-4f5e-84f8-f574c844b929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d9b42-7fa8-400d-938b-1d48837495a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
